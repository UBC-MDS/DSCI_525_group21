{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "#import intake\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import xarray as xr\n",
    "from urllib.request import urlretrieve\n",
    "#import proplot as pplot\n",
    "#from joblib import Parallel, delayed\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")  # ignore some annoying matplotlib warnings\n",
    "from memory_profiler import memory_usage\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schaf\\miniconda3\\envs\\525\\lib\\site-packages\\rpy2\\robjects\\packages.py:366: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# more library loading\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figshare/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata output\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # \n",
    "files = data[\"files\"] # we only want the data and readme 'name' key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#download readme and data.zip files only\n",
    "files_to_dl = [\"README.md\", \"data.zip\"]\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#extract zip files to repo\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Combining data CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 137.07 MiB, increment: 0.23 MiB\n",
      "Processing the file ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "Processing the file ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "Processing the file AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "Processing the file BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "Processing the file BCC-ESM1_daily_rainfall_NSW.csv\n",
      "Processing the file CanESM5_daily_rainfall_NSW.csv\n",
      "Processing the file CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "Processing the file CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "Processing the file CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "Processing the file EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "Processing the file FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "Processing the file FGOALS-g3_daily_rainfall_NSW.csv\n",
      "Processing the file GFDL-CM4_daily_rainfall_NSW.csv\n",
      "Processing the file GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "Processing the file INM-CM4-8_daily_rainfall_NSW.csv\n",
      "Processing the file INM-CM5-0_daily_rainfall_NSW.csv\n",
      "Processing the file KIOST-ESM_daily_rainfall_NSW.csv\n",
      "Processing the file MIROC6_daily_rainfall_NSW.csv\n",
      "Processing the file MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "Processing the file MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "Processing the file MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "Processing the file MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "Processing the file NESM3_daily_rainfall_NSW.csv\n",
      "Processing the file NorESM2-LM_daily_rainfall_NSW.csv\n",
      "Processing the file NorESM2-MM_daily_rainfall_NSW.csv\n",
      "Processing the file SAM0-UNICON_daily_rainfall_NSW.csv\n",
      "Processing the file TaiESM1_daily_rainfall_NSW.csv\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "\n",
    "# Shows time that regular python takes to merge file\n",
    "# Join all data together\n",
    "import pandas as pd\n",
    "use_cols = [\"time\",'lat_min','lat_max','lon_min', 'lon_max', 'rain (mm/day)']\n",
    "\n",
    "files = glob.glob('./figshare/*.csv')\n",
    "df_all = None\n",
    "\n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    if '_daily_rainfall_NSW.csv' in filename:\n",
    "        print(f\"Processing the file {filename}\")\n",
    "        model = filename.split('_daily_rainfall_NSW.csv')[0]\n",
    "\n",
    "        df = pd.read_csv(file, usecols=use_cols, index_col=0)\n",
    "        df['model'] = model    \n",
    "\n",
    "        if df_all is None:\n",
    "            df_all = df\n",
    "        else:\n",
    "            df_all = df_all.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined file\n",
    "df_all.to_csv('./figshare/combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7G\tfigshare/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "#get file size of combined csv\n",
    "du -sh figshare/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "Our team members had the following computer specs:  \n",
    "\n",
    "| Team Member       | Ram     | Processor     |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|  Cal | 16GB   | AMD Ryzen 5 3600 6-core    |\n",
    "| Justin  | 32GB  | Intel i5 | \n",
    "| Anita   |  12GB | Intel i5  | \n",
    "| Yuan  |  8GB | Intel i5  | \n",
    "\n",
    "We used the Pandas default writing method, and found the processing times and peak memory usage to vary by member as: \n",
    "\n",
    "| Team Member       |  Processing Time     | Peak Memory Usage |\n",
    "| :------------- | :---------- | :-----------: |\n",
    "|  Cal |  1min 13sec  | 137 mb |\n",
    "|  Justin |  1min 07sec  | 1593 mb |\n",
    "|  Anita |  3min 34sec  | 9050 mb |\n",
    "|  Yuan |  9sec (using Dash)  | 250 mb |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance using Default Pandas Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "FGOALS-f3-L         3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 11281.64 MiB, increment: 7587.66 MiB\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# time to read/calculate when using default Pandas method\n",
    "df = pd.read_csv(\"figshare/combined_data.csv\")\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance when loading in Select Columns only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "FGOALS-f3-L         3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 11356.05 MiB, increment: 4572.77 MiB\n",
      "Wall time: 51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "use_cols = [\"time\", \"rain (mm/day)\", \"model\"]\n",
    "df = pd.read_csv(\"figshare/combined_data.csv\", usecols = use_cols)\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance when reading file using chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# 10 million chunk size\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"figshare/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# 1 million chunk size\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"figshare/combined_data.csv\", chunksize=1_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"figshare/combined_data.csv\", chunksize=500_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance when loading with simplier data types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "col_type = {'time':object, 'lat_min':np.float32, 'lat_max':np.float32, 'lon_min': np.float32, \n",
    "           'lon_max': np.float32, 'rain (mm/day)': np.float32, 'model': object}\n",
    "df2 = pd.read_csv(\"figshare/combined_data.csv\", dtype=col_type)\n",
    "print(df2[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*observations*\n",
    "\n",
    "When we read in our combined_data .csv and perform simple EDA (using Pandas), our processing time took 1min:07sec to 3min:34sec (using dash took 9 seconds for one of our team members).\n",
    "\n",
    "We found that processing and peak memory usage improved when we employed the following methods:\n",
    "\n",
    "**Changing the Data Type of the Columns**\n",
    "\n",
    "- Here we used float32 rather than float64 for 4 out of the 7 columns\n",
    "\n",
    "| Team Member       |  Processing Time     | Peak Memory Usage |\n",
    "| :------------- | :---------- | :-----------: |\n",
    "|  Cal |   |  |\n",
    "|  Justin |  x  | x |\n",
    "|  Anita |  x  | x |\n",
    "|  Yuan |  x  | x |\n",
    "\n",
    "\n",
    "**Reading in Fewer Columns**\n",
    "\n",
    "- Here we read in only 3 out of the original 7 columns (date, rain, model) \n",
    "\n",
    "| Team Member       |  Processing Time     | Peak Memory Usage |\n",
    "| :------------- | :---------- | :-----------: |\n",
    "|  Cal |  53 sec  | 11.36GB |\n",
    "|  Justin |  x  | x |\n",
    "|  Anita |  x  | x |\n",
    "|  Yuan |  x  | x |\n",
    "\n",
    "\n",
    "**Reading in Using Chunks**\n",
    "\n",
    "- We read in the file using several chunk size options (0.5mil, 1mil, 10mil). \n",
    "| Team Member       |  Processing Time     | Peak Memory Usage |\n",
    "| :------------- | :---------- | :-----------: |\n",
    "|  Cal |  x  | x |\n",
    "|  Justin |  x  | x |\n",
    "|  Anita |  x  | x |\n",
    "|  Yuan |  x  | x |\n",
    "\n",
    "- We found that with larger chunk sizes, more peak memory usage is required to handle it. With smaller chunk sizes, CPU processing time was reduced. \n",
    "\n",
    "- 500_000 chunks: peak memory: 4424.06 MiB, increment: 0.00 MiB, CPU_time: total: 1min 2s\n",
    "- 1_000_000 chunks: peak memory: 4595.90 MiB, increment: 82.60 MiB, CPU_time: 58.7s\n",
    "- 10_000_000 chunks: peak memory: 6511.22 MiB, increment: 2100.35 MiB, CPU_time: 53.9s\n",
    "\n",
    "**Loading with Dash**\n",
    "\n",
    "| Team Member       |  Processing Time     | Peak Memory Usage |\n",
    "| :------------- | :---------- | :-----------: |\n",
    "|  Cal |  x  | x |\n",
    "|  Justin |  x  | x |\n",
    "|  Anita |  x  | x |\n",
    "|  Yuan |  x  | x |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## install the packages https://arrow.apache.org/docs/python/install.html\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "## How to install put instructions https://anaconda.org/conda-forge/rpy2\n",
    "import rpy2.rinterface\n",
    "# install this https://pypi.org/project/rpy2-arrow/#description  pip install rpy2-arrow\n",
    "# have to install this as well conda install -c conda-forge r-arrow \n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "### instruction\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "#just seeing if its available\n",
    "library(\"arrow\")\n",
    "library(\"dplyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4137.67 MiB, increment: 3874.96 MiB\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## read more on the datasets here  https://arrow.apache.org/docs/python/dataset.html\n",
    "dataset = ds.dataset(\"figshare/combined_data.csv\", format=\"csv\")\n",
    "## this is of arrow table format\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# experiment in writing in feather format \n",
    "#feather.write_feather(table, 'figshare/figshare.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "### her we are showing how much time it took to read a feather file what we wrote in python\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"figshare/figshare.feather\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**observation**\n",
    "\n",
    "The following table shows how the reading in, and performing of simple EDA (i.e., getting the number of observations per model) varied based on our team members and using default Pandas versus using Feather for R:\n",
    "\n",
    "| Team Member       | Processing Time (Feather vs Pandas)  | Peak Memory Usage (Feather vs Pandas)  |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|  Cal |  x  | x    |\n",
    "| Justin  | x  | x | \n",
    "| Anita   |  x |x  | \n",
    "| Yuan  |  x |x  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why I chose Feather?\n",
    "\n",
    "I chose the feather data format - which was designed to improve data interoperability (ex. language agnostic - R and Python) while dealing with columnar tabular data. I also wanted the fastest load and save times. Feather is a fast, lightweight, and easy-to-use binary file format. Feather also doesn't use compression internally and works best with SSD drives (which my laptop has).\n",
    "\n",
    "However, if I needed to store this for long term storage, I would not use Feather because the file format is relatively new and can change. Feather does not guarantee if the format will stay the same between versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
